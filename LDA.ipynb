{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.utils import simple_preprocess\n",
    "import numpy as np\n",
    "import nltk \n",
    "import pandas as pd\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/xmliszt/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/xmliszt/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os,glob\n",
    "import string\n",
    "import array\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "train_docs=[]\n",
    "test_docs=[]\n",
    "def clean(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "        words = list(map(stem_text, words))\n",
    "        return words\n",
    "    \n",
    "def create_docs(path,documents):\n",
    "    for filename in glob.glob(os.path.join(path, '*.txt')):\n",
    "       with open(os.path.join(os.getcwd(), filename), 'r') as f: # open in readonly mode\n",
    "          # do your stuff\n",
    "            text = f.read()\n",
    "            documents.append(clean(text))\n",
    "            f.close()\n",
    "\n",
    "path = '/home/xmliszt/Documents/git/capstone-topic-modelling/BBC News Summary/Summaries/business'\n",
    "create_docs(path,train_docs)\n",
    "path = '/home/xmliszt/Documents/git/capstone-topic-modelling/BBC News Summary/Summaries/entertainment'\n",
    "create_docs(path,train_docs)\n",
    "path = '/home/xmliszt/Documents/git/capstone-topic-modelling/BBC News Summary/Summaries/politics'\n",
    "create_docs(path,train_docs)\n",
    "path = '/home/xmliszt/Documents/git/capstone-topic-modelling/BBC News Summary/Summaries/sport'\n",
    "create_docs(path,train_docs)\n",
    "path = '/home/xmliszt/Documents/git/capstone-topic-modelling/BBC News Summary/Summaries/tech'\n",
    "create_docs(path,train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['howev',\n",
       "  'slowerthanexpect',\n",
       "  'fourth',\n",
       "  'quarter',\n",
       "  'point',\n",
       "  'modest',\n",
       "  'growth',\n",
       "  'tradedriven',\n",
       "  'economi',\n",
       "  'global',\n",
       "  'technolog',\n",
       "  'demand',\n",
       "  'fall',\n",
       "  'backth',\n",
       "  'economi',\n",
       "  'poor',\n",
       "  'perform',\n",
       "  'juli',\n",
       "  'septemb',\n",
       "  'period',\n",
       "  'follow',\n",
       "  'four',\n",
       "  'consecut',\n",
       "  'quarter',\n",
       "  'doubledigit',\n",
       "  'growth',\n",
       "  'singapor',\n",
       "  'bounc',\n",
       "  'back',\n",
       "  'strongli',\n",
       "  'effect',\n",
       "  'deadli',\n",
       "  'sar',\n",
       "  'viru',\n",
       "  'advanc',\n",
       "  'secondfastest',\n",
       "  'asia',\n",
       "  'china',\n",
       "  'led',\n",
       "  'growth',\n",
       "  'kei',\n",
       "  'manufactur',\n",
       "  'sector',\n",
       "  'surpris',\n",
       "  'weak',\n",
       "  'fourth',\n",
       "  'quarter',\n",
       "  'numberthat',\n",
       "  'third',\n",
       "  'quarter',\n",
       "  'fell',\n",
       "  'well',\n",
       "  'analyst',\n",
       "  'forecast'],\n",
       " ['prosecut',\n",
       "  'star',\n",
       "  'wit',\n",
       "  'former',\n",
       "  'worldcom',\n",
       "  'financi',\n",
       "  'chief',\n",
       "  'scott',\n",
       "  'sullivan',\n",
       "  'said',\n",
       "  'mr',\n",
       "  'ebber',\n",
       "  'order',\n",
       "  'account',\n",
       "  'adjust',\n",
       "  'firm',\n",
       "  'tell',\n",
       "  'hit',\n",
       "  'book',\n",
       "  'howev',\n",
       "  'ms',\n",
       "  'cooper',\n",
       "  'said',\n",
       "  'mr',\n",
       "  'sullivan',\n",
       "  'mention',\n",
       "  'anyth',\n",
       "  'uncomfort',\n",
       "  'worldcom',\n",
       "  'account',\n",
       "  'audit',\n",
       "  'committe',\n",
       "  'meetingmr',\n",
       "  'ebber',\n",
       "  'lawyer',\n",
       "  'said',\n",
       "  'unawar',\n",
       "  'fraud',\n",
       "  'argu',\n",
       "  'auditor',\n",
       "  'alert',\n",
       "  'problemsmr',\n",
       "  'ebber',\n",
       "  'plead',\n",
       "  'guilti',\n",
       "  'charg',\n",
       "  'fraud',\n",
       "  'conspiracym',\n",
       "  'cooper',\n",
       "  'said',\n",
       "  'sharehold',\n",
       "  'meet',\n",
       "  'mr',\n",
       "  'ebber',\n",
       "  'often',\n",
       "  'pass',\n",
       "  'technic',\n",
       "  'question',\n",
       "  'compani',\n",
       "  'financ',\n",
       "  'chief',\n",
       "  'give',\n",
       "  'brief',\n",
       "  'answer',\n",
       "  'himselfprosecut',\n",
       "  'lawyer',\n",
       "  'argu',\n",
       "  'mr',\n",
       "  'ebber',\n",
       "  'orchestr',\n",
       "  'seri',\n",
       "  'account',\n",
       "  'trick',\n",
       "  'worldcom',\n",
       "  'order',\n",
       "  'employe',\n",
       "  'hide',\n",
       "  'expens',\n",
       "  'inflat',\n",
       "  'revenu',\n",
       "  'meet',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'earn',\n",
       "  'estim'],\n",
       " ['pension',\n",
       "  'benefit',\n",
       "  'guaranti',\n",
       "  'corpor',\n",
       "  'pbgc',\n",
       "  'deficit',\n",
       "  'financi',\n",
       "  'economist',\n",
       "  'roundtabl',\n",
       "  'fer',\n",
       "  'want',\n",
       "  'congress',\n",
       "  'actth',\n",
       "  'compani',\n",
       "  'estim',\n",
       "  'hole',\n",
       "  'pilot',\n",
       "  'pension',\n",
       "  'scheme',\n",
       "  'pbgc',\n",
       "  'guaranteeth',\n",
       "  'fer',\n",
       "  'sai',\n",
       "  'firm',\n",
       "  'allow',\n",
       "  'reduc',\n",
       "  'insur',\n",
       "  'premium',\n",
       "  'pai',\n",
       "  'pbgc',\n",
       "  'fund',\n",
       "  'littl',\n",
       "  'firmer',\n",
       "  'hand',\n",
       "  'pension',\n",
       "  'issu',\n",
       "  'us',\n",
       "  'congress',\n",
       "  'avoid',\n",
       "  'turn',\n",
       "  'taxpay',\n",
       "  'instead',\n",
       "  'turn',\n",
       "  'oblig',\n",
       "  'back',\n",
       "  'onto',\n",
       "  'compani',\n",
       "  'deserv',\n",
       "  'pai',\n",
       "  'said',\n",
       "  'professor',\n",
       "  'denni',\n",
       "  'logu',\n",
       "  'dean',\n",
       "  'price',\n",
       "  'colleg',\n",
       "  'busi',\n",
       "  'univers',\n",
       "  'oklahomainstead',\n",
       "  'taxpay',\n",
       "  'pick',\n",
       "  'bill',\n",
       "  'fer',\n",
       "  'want',\n",
       "  'congressmen',\n",
       "  'chang',\n",
       "  'pbgc',\n",
       "  'fund',\n",
       "  'rule'],\n",
       " ['retail',\n",
       "  'sale',\n",
       "  'figur',\n",
       "  'weak',\n",
       "  'bank',\n",
       "  'england',\n",
       "  'governor',\n",
       "  'mervyn',\n",
       "  'king',\n",
       "  'indic',\n",
       "  'last',\n",
       "  'night',\n",
       "  'nt',\n",
       "  'realli',\n",
       "  'accur',\n",
       "  'impress',\n",
       "  'christma',\n",
       "  'trade',\n",
       "  'easter',\n",
       "  'said',\n",
       "  'mr',\n",
       "  'shawth',\n",
       "  'last',\n",
       "  'time',\n",
       "  'retail',\n",
       "  'endur',\n",
       "  'tougher',\n",
       "  'christma',\n",
       "  'year',\n",
       "  'previous',\n",
       "  'sale',\n",
       "  'plung',\n",
       "  'number',\n",
       "  'retail',\n",
       "  'alreadi',\n",
       "  'report',\n",
       "  'poor',\n",
       "  'figur',\n",
       "  'decemberretail',\n",
       "  'sale',\n",
       "  'drop',\n",
       "  'month',\n",
       "  'decemb',\n",
       "  'rise',\n",
       "  'novemb',\n",
       "  'offic',\n",
       "  'nation',\n",
       "  'statist',\n",
       "  'on',\n",
       "  'saidcloth',\n",
       "  'retail',\n",
       "  'nonspecialist',\n",
       "  'store',\n",
       "  'worst',\n",
       "  'hit',\n",
       "  'internet',\n",
       "  'retail',\n",
       "  'show',\n",
       "  'signific',\n",
       "  'growth',\n",
       "  'accord',\n",
       "  'onsuk',\n",
       "  'retail',\n",
       "  'sale',\n",
       "  'fell',\n",
       "  'decemb',\n",
       "  'fail',\n",
       "  'meet',\n",
       "  'expect',\n",
       "  'make',\n",
       "  'count',\n",
       "  'worst',\n",
       "  'christma',\n",
       "  'sinc',\n",
       "  'retail',\n",
       "  'includ',\n",
       "  'hmv',\n",
       "  'monsoon',\n",
       "  'jessop',\n",
       "  'bodi',\n",
       "  'shop',\n",
       "  'tesco',\n",
       "  'report',\n",
       "  'festiv',\n",
       "  'sale',\n",
       "  'well',\n",
       "  'last',\n",
       "  'year']]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "train_docs[1:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word 3 (\"export\") appears 1 time.\nWord 11 (\"kept\") appears 1 time.\nWord 15 (\"oil\") appears 1 time.\nWord 18 (\"price\") appears 1 time.\nWord 23 (\"strong\") appears 1 time.\nWord 31 (\"china\") appears 1 time.\nWord 35 (\"fall\") appears 1 time.\nWord 41 (\"growth\") appears 1 time.\nWord 48 (\"point\") appears 1 time.\nWord 60 (\"account\") appears 2 time.\nWord 68 (\"chief\") appears 1 time.\nWord 75 (\"financ\") appears 2 time.\nWord 82 (\"hit\") appears 1 time.\nWord 85 (\"meet\") appears 1 time.\nWord 87 (\"mr\") appears 2 time.\nWord 113 (\"deficit\") appears 2 time.\nWord 116 (\"fund\") appears 1 time.\nWord 133 (\"turn\") appears 1 time.\nWord 135 (\"us\") appears 6 time.\nWord 170 (\"sinc\") appears 1 time.\nWord 174 (\"trade\") appears 1 time.\nWord 196 (\"mondai\") appears 1 time.\nWord 209 (\"share\") appears 1 time.\nWord 217 (\"ad\") appears 1 time.\nWord 237 (\"japan\") appears 2 time.\nWord 246 (\"seen\") appears 1 time.\nWord 251 (\"world\") appears 1 time.\nWord 278 (\"trend\") appears 1 time.\nWord 291 (\"much\") appears 1 time.\nWord 318 (\"afford\") appears 1 time.\nWord 319 (\"alan\") appears 1 time.\nWord 320 (\"asian\") appears 3 time.\nWord 321 (\"attempt\") appears 1 time.\nWord 322 (\"balanc\") appears 1 time.\nWord 323 (\"bond\") appears 1 time.\nWord 324 (\"bridg\") appears 1 time.\nWord 325 (\"close\") appears 1 time.\nWord 326 (\"come\") appears 1 time.\nWord 327 (\"cover\") appears 1 time.\nWord 328 (\"current\") appears 2 time.\nWord 329 (\"debt\") appears 1 time.\nWord 330 (\"dollar\") appears 7 time.\nWord 331 (\"fed\") appears 1 time.\nWord 332 (\"gap\") appears 3 time.\nWord 333 (\"given\") appears 1 time.\nWord 334 (\"govern\") appears 1 time.\nWord 335 (\"help\") appears 2 time.\nWord 336 (\"invest\") appears 1 time.\nWord 337 (\"let\") appears 1 time.\nWord 338 (\"minist\") appears 2 time.\nWord 339 (\"must\") appears 1 time.\nWord 340 (\"noth\") appears 1 time.\nWord 341 (\"perhap\") appears 1 time.\nWord 342 (\"polici\") appears 1 time.\nWord 343 (\"present\") appears 1 time.\nWord 344 (\"public\") appears 1 time.\nWord 345 (\"reserv\") appears 1 time.\nWord 346 (\"seem\") appears 1 time.\nWord 347 (\"sell\") appears 1 time.\nWord 348 (\"size\") appears 1 time.\nWord 349 (\"soon\") appears 1 time.\nWord 350 (\"spend\") appears 1 time.\nWord 351 (\"state\") appears 2 time.\nWord 352 (\"stop\") appears 1 time.\nWord 353 (\"support\") appears 1 time.\nWord 354 (\"talk\") appears 1 time.\nWord 355 (\"tradit\") appears 1 time.\nWord 356 (\"valu\") appears 1 time.\nWord 357 (\"warn\") appears 1 time.\nWord 358 (\"whole\") appears 1 time.\nWord 359 (\"whose\") appears 1 time.\nWord 360 (\"word\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(train_docs)\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)\n",
    "#     count += 1\n",
    "#     if count > 10:\n",
    "#         break\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "#For each document we create a dictionary reporting how many words and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in train_docs]\n",
    "bow_doc_10 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_10)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_10[i][0], \n",
    "                                               dictionary[bow_doc_10[i][0]], \n",
    "bow_doc_10[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 0.11104392088587417),\n (1, 0.12409600975358015),\n (2, 0.11021474446002884),\n (3, 0.1681205051403281),\n (4, 0.1440329094089247),\n (5, 0.11903882545112768),\n (6, 0.17421638093068081),\n (7, 0.11672755837914486),\n (8, 0.184649872323546),\n (9, 0.16485744925951995),\n (10, 0.1316827363354427),\n (11, 0.16485744925951995),\n (12, 0.105565434141836),\n (13, 0.47981962852214355),\n (14, 0.12042087087136809),\n (15, 0.1440329094089247),\n (16, 0.1863848761002872),\n (17, 0.19009347123018325),\n (18, 0.12007094223769688),\n (19, 0.3596965555914307),\n (20, 0.17166764882943514),\n (21, 0.11132410367616014),\n (22, 0.12856247021713252),\n (23, 0.12566414291044306),\n (24, 0.23672996945805594),\n (25, 0.16382533247295072),\n (26, 0.31110431528565613),\n (27, 0.04509917690726134)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Topic: 0 Word: 0.004*\"mr\" + 0.004*\"us\" + 0.003*\"year\" + 0.003*\"economi\" + 0.003*\"tax\" + 0.003*\"bank\" + 0.003*\"peopl\" + 0.003*\"growth\" + 0.003*\"govern\" + 0.003*\"price\"\nTopic: 1 Word: 0.004*\"win\" + 0.004*\"player\" + 0.004*\"game\" + 0.003*\"mr\" + 0.003*\"plai\" + 0.003*\"england\" + 0.003*\"two\" + 0.003*\"six\" + 0.003*\"number\" + 0.003*\"rugbi\"\nTopic: 2 Word: 0.007*\"film\" + 0.004*\"best\" + 0.004*\"firm\" + 0.003*\"mr\" + 0.003*\"new\" + 0.003*\"us\" + 0.003*\"sale\" + 0.003*\"year\" + 0.002*\"director\" + 0.002*\"compani\"\nTopic: 3 Word: 0.006*\"mobil\" + 0.005*\"music\" + 0.004*\"us\" + 0.004*\"phone\" + 0.003*\"sale\" + 0.003*\"show\" + 0.003*\"peopl\" + 0.003*\"new\" + 0.003*\"digit\" + 0.003*\"network\"\nTopic: 4 Word: 0.006*\"film\" + 0.006*\"mr\" + 0.005*\"game\" + 0.004*\"blair\" + 0.004*\"lord\" + 0.003*\"plai\" + 0.003*\"brown\" + 0.003*\"labour\" + 0.003*\"howard\" + 0.003*\"first\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify docs in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nScore: 0.9829461574554443\t \nTopic: 0.004*\"mr\" + 0.004*\"us\" + 0.003*\"year\" + 0.003*\"economi\" + 0.003*\"tax\" + 0.003*\"bank\" + 0.003*\"peopl\" + 0.003*\"growth\" + 0.003*\"govern\" + 0.003*\"price\"\n[0.98294616]\n0\n"
     ]
    }
   ],
   "source": [
    "score_list=[]\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[500]], key=lambda tup: -1*tup[1]):\n",
    "    score_list.append(score)\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "for i in range(len(score_list)):\n",
    "    if score_list[i] == max(score_list):\n",
    "        maxIndex = i\n",
    "    else:\n",
    "        pass\n",
    "print(score_list)\n",
    "print(maxIndex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score: 0.5156767964363098\t Topic: 0.006*\"mobil\" + 0.005*\"music\" + 0.004*\"us\" + 0.004*\"phone\" + 0.003*\"sale\" + 0.003*\"show\" + 0.003*\"peopl\"\nScore: 0.375169962644577\t Topic: 0.004*\"mr\" + 0.004*\"us\" + 0.003*\"year\" + 0.003*\"economi\" + 0.003*\"tax\" + 0.003*\"bank\" + 0.003*\"peopl\"\nScore: 0.10413332283496857\t Topic: 0.006*\"film\" + 0.006*\"mr\" + 0.005*\"game\" + 0.004*\"blair\" + 0.004*\"lord\" + 0.003*\"plai\" + 0.003*\"brown\"\n"
     ]
    }
   ],
   "source": [
    "# unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "unseen_document = 'While the idea was popular amongst the children, it initially received some resistance from the teachers and there were problems with laptops getting broken. However, Mr Negroponte has adapted the idea to his own work in Cambodia where he set up two schools together with his wife and gave the children laptops. \"We put in 25 laptops three years ago , only one has been broken, the kids cherish these things, its also a TV a telephone and a games machine, not just a textbook.\" Mr Negroponte wants the laptops to become more common than mobile phones but conceded this was ambitious. Nokia make 200 million cell phones a year, so for us to claim were going to make 200 million laptops is a big number, but were not talking about doing it in three or five years, were talking about months. He plans to be distributing them by the end of 2006 and is already in discussion with the Chinese education ministry who are expected to make a large order. In China they spend 17 per child per year on textbooks. Thats for five or six years, so if we can distribute and sell laptops in quantities of one million or more to ministries of education that cheaper'\n",
    "bow_vector = dictionary.doc2bow(clean(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model_tfidf.print_topic(index, 7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Wordcloud of Top N words in each topic\n",
    "# from matplotlib import pyplot as plt\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "# from nltk.corpus import stopwords\n",
    "# import matplotlib.colors as mcolors\n",
    "\n",
    "# cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "# cloud = WordCloud(stopwords = set(stopwords.words('english')),\n",
    "#                   background_color='white',\n",
    "#                   width=2500,\n",
    "#                   height=1800,\n",
    "#                   max_words=20,\n",
    "#                   colormap='tab10',\n",
    "#                   color_func=lambda *args, **kwargs: cols[i],\n",
    "#                   prefer_horizontal=1.0)\n",
    "\n",
    "# topics = lda_model_tfidf.show_topics(formatted=False)\n",
    "\n",
    "# fig, axes = plt.subplots(1, 5, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "# for i, ax in enumerate(axes.flatten()):\n",
    "#     fig.add_subplot(ax)\n",
    "#     topic_words = dict(topics[i][1])\n",
    "#     cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "#     plt.gca().imshow(cloud)\n",
    "#     plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "#     plt.gca().axis('off')\n",
    "\n",
    "\n",
    "# plt.subplots_adjust(wspace=0, hspace=0)\n",
    "# plt.axis('off')\n",
    "# plt.margins(x=0, y=0)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2220, 1)\n[t-SNE] Computing 91 nearest neighbors...\n[t-SNE] Indexed 2220 samples in 0.001s...\n[t-SNE] Computed neighbors for 2220 samples in 0.020s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 2220\n[t-SNE] Computed conditional probabilities for sample 2000 / 2220\n[t-SNE] Computed conditional probabilities for sample 2220 / 2220\n[t-SNE] Mean sigma: 0.000358\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7e376547660e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# tSNE Dimension Reduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mtsne_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtsne_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# # Plot the Topic Clusters using Bokeh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/capstone-topic-modelling/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \"\"\"\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/capstone-topic-modelling/venv/lib/python3.8/site-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    823\u001b[0m             pca = PCA(n_components=self.n_components, svd_solver='randomized',\n\u001b[1;32m    824\u001b[0m                       random_state=random_state)\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mX_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;31m# The embedding is initialized with iid samples from Gaussians with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/capstone-topic-modelling/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git/capstone-topic-modelling/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'arpack'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'randomized'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_truncated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_svd_solver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             raise ValueError(\"Unrecognized svd_solver='{0}'\"\n",
      "\u001b[0;32m~/Documents/git/capstone-topic-modelling/venv/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit_truncated\u001b[0;34m(self, X, n_components, svd_solver)\u001b[0m\n\u001b[1;32m    505\u001b[0m                              % (n_components, svd_solver))\n\u001b[1;32m    506\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             raise ValueError(\"n_components=%r must be between 1 and \"\n\u001b[0m\u001b[1;32m    508\u001b[0m                              \u001b[0;34m\"min(n_samples, n_features)=%r with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m                              \u001b[0;34m\"svd_solver='%s'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=2 must be between 1 and min(n_samples, n_features)=1 with svd_solver='randomized'"
     ]
    }
   ],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "for i, row_list in enumerate(lda_model_tfidf[bow_corpus]):\n",
    "    row_list=sorted(row_list, key=lambda tup: -1*tup[1])\n",
    "    topic_weights.append(row_list[0][1])\n",
    "# print(topic_weights)\n",
    "\n",
    "# Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "\n",
    "# Keep the well separated points (optional)\n",
    "arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "print(arr.shape)\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "\n",
    "# # Plot the Topic Clusters using Bokeh\n",
    "import matplotlib.colors as mcolors\n",
    "output_notebook()\n",
    "n_topics = 4\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "language": "python",
   "name": "python38564bitvenvb8743f5ccd1f4aa1812964bc496a952b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}